{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for retrieval using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents:\n",
      "Filename: 186493425728.txt, Similarity Score: 0.6510401386173539\n",
      "Filename: 266951087904.txt, Similarity Score: 0.5819787986445043\n",
      "Filename: 325900456128.txt, Similarity Score: 0.5668385238694112\n",
      "Filename: 274558530999.txt, Similarity Score: 0.558699038160222\n",
      "Filename: 153704752472.txt, Similarity Score: 0.537042624440397\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load documents and filenames\n",
    "folder_path = \"description\"  # Replace with your folder path\n",
    "documents = []\n",
    "filenames = []  # To store filenames\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        filenames.append(file_name)  # Save the filename\n",
    "        with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:\n",
    "            documents.append(file.read())\n",
    "\n",
    "# Step 2: Define the query\n",
    "query = \"Brown Men's Shirt\"\n",
    "\n",
    "# Step 3: Preprocess and compute TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "query_vector = vectorizer.transform([query])\n",
    "\n",
    "# Step 4: Compute similarity\n",
    "cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "# Step 5: Retrieve top 5 documents\n",
    "top_5_indices = cosine_similarities.argsort()[-5:][::-1]  # Indices of top 5 similar documents\n",
    "top_5_results = [(filenames[i], cosine_similarities[i]) for i in top_5_indices]  # Pair filename with score\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 5 documents:\")\n",
    "for filename, score in top_5_results:\n",
    "    print(f\"Filename: {filename}, Similarity Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/t2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/t2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents:\n",
      "Filename: 186493425728.txt, BM25 Score: 17.09573370876749\n",
      "Filename: 266951087904.txt, BM25 Score: 12.643704065747965\n",
      "Filename: 153704752472.txt, BM25 Score: 12.132353422352997\n",
      "Filename: 325900456128.txt, BM25 Score: 11.738037763447402\n",
      "Filename: 371061579939.txt, BM25 Score: 11.63897420229748\n"
     ]
    }
   ],
   "source": [
    "# Ensure nltk resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Load documents and filenames\n",
    "folder_path = \"description\"  # Replace with your folder path\n",
    "documents = []\n",
    "filenames = []  # To store filenames\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        filenames.append(file_name)  # Save the filename\n",
    "        with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:\n",
    "            documents.append(file.read())\n",
    "\n",
    "# Step 2: Preprocess documents\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove punctuation and stopwords\n",
    "    return tokens\n",
    "\n",
    "tokenized_documents = [preprocess(doc) for doc in documents]\n",
    "\n",
    "# Step 3: Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_documents)\n",
    "\n",
    "# Step 4: Define and preprocess the query\n",
    "query = \"Brown Men's Shirt\"\n",
    "tokenized_query = preprocess(query)\n",
    "\n",
    "# Step 5: Compute BM25 scores\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "# Step 6: Retrieve top 5 documents\n",
    "top_5_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:5]\n",
    "top_5_results = [(filenames[i], scores[i]) for i in top_5_indices]\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 5 documents:\")\n",
    "for filename, score in top_5_results:\n",
    "    print(f\"Filename: {filename}, BM25 Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 documents:\n",
      "Filename: 186493425728.txt, Similarity Score: 0.515089750289917\n",
      "Filename: 266951087904.txt, Similarity Score: 0.4705897271633148\n",
      "Filename: 156264203608.txt, Similarity Score: 0.4406551122665405\n",
      "Filename: 371061579939.txt, Similarity Score: 0.4157381057739258\n",
      "Filename: 153704752472.txt, Similarity Score: 0.4154118299484253\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load documents and filenames\n",
    "folder_path = \"description\"  # Replace with your folder path\n",
    "documents = []\n",
    "filenames = []  # To store filenames\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        filenames.append(file_name)  # Save the filename\n",
    "        with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:\n",
    "            documents.append(file.read())\n",
    "\n",
    "# Step 2: Load SentenceBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # A lightweight, fast model for embedding\n",
    "\n",
    "# Step 3: Compute embeddings for documents\n",
    "document_embeddings = model.encode(documents, convert_to_tensor=True)\n",
    "\n",
    "# Step 4: Define and compute embedding for the query\n",
    "query = \"Brown Men's Shirt\"\n",
    "query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# Step 5: Compute cosine similarity\n",
    "cosine_similarities = util.cos_sim(query_embedding, document_embeddings).flatten()\n",
    "\n",
    "# Step 6: Retrieve top 5 documents\n",
    "top_5_indices = cosine_similarities.argsort(descending=True)[:5]\n",
    "top_5_results = [(filenames[i], cosine_similarities[i].item()) for i in top_5_indices]\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 5 documents:\")\n",
    "for filename, score in top_5_results:\n",
    "    print(f\"Filename: {filename}, Similarity Score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tgn2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
